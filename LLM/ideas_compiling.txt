The scrambing ideas for the the machine learning addition to the website as the hosting machine will be very expensive for the full machine 
-- use two instance , one for hsoting the instance , one for the GPU support in aws -- G4, G5, P4 instance 
-- these machine have GPU with choosing the ubuntu cuda installed images 




-- I can use SSH to work on the machine 
-- the other machine can talk to the GPU machine GRPC web client for bidiretional communication 
-- aws lambda can be used for machine sleeping and waking up calls 


-- For the code as insfrastructure I can use combination of terraform and ansible to deploy the code and run the production machine 
-- grpc protocals can be use to fetch the data from one machine to other machine 



reqs --

-- pip install transformers

for the LLM and ML tasks 

-- the hugging face models can be a starting point to deep dive 
-- LLM caching for faster responses 
-- using langchain to build the models 
-- using the SLM 
-- 






small models list -- 

distilBert 
gpt-neo 
Tiny-bert 
mobile-bert 
phi-3, phi-2
orca 2 



links --


https://huggingface.co/collections/open-llm-leaderboard/llm-leaderboard-best-models-652d6c7965a4619fb5c27a03
https://www.salesforce.com/blog/small-language-models/
https://medium.com/@nageshmashette32/small-language-models-slms-305597c9edf2
https://deepgram.com/ai-glossary/distilbert
https://www.datacamp.com/tutorial/phi-3-tutorial





import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed


set_seed(2024)  

prompt = "Africa is an emerging economy because"

model_checkpoint = "microsoft/Phi-3-mini-4k-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_checkpoint,
                                             trust_remote_code=True,
                                             torch_dtype="auto",
                                             device_map="cuda")

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, do_sample=True, max_new_tokens=120)
response= tokenizer.decode(outputs[0], skip_special_tokens=True)



print(response)
