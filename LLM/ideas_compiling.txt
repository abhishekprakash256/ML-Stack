The scrambing ideas for the the machine learning addition to the website as the hosting machine will be very expensive for the full machine 
-- use two instance , one for hsoting the instance , one for the GPU support in aws -- G4, G5, P4 instance 
-- these machine have GPU with choosing the ubuntu cuda installed images 




-- I can use SSH to work on the machine 
-- the other machine can talk to the GPU machine GRPC web client for bidiretional communication 
-- aws lambda can be used for machine sleeping and waking up calls 


-- For the code as insfrastructure I can use combination of terraform and ansible to deploy the code and run the production machine 
-- grpc protocals can be use to fetch the data from one machine to other machine 



reqs --

-- pip install transformers

for the LLM and ML tasks 

-- the hugging face models can be a starting point to deep dive 
-- LLM caching for faster responses 
-- using langchain to build the models 
-- using the SLM 
-- 




links --


https://huggingface.co/collections/open-llm-leaderboard/llm-leaderboard-best-models-652d6c7965a4619fb5c27a03
https://www.salesforce.com/blog/small-language-models/



small models list -- 

distilBert 
gpt-neo 
Tiny-bert 
mobile-bert 
